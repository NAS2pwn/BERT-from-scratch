{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam classifier using BERT\n",
    "\n",
    "Ce projet est pédagogique, je souhaite comprendre le fonctionnement de BERT et son utilisation pour un classifieur de spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit le processeur à utiliser, si on a un GPU on l'utilise, sinon on utilise le CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'importe le dataset, le l'encode en latin-1 pour éviter les erreurs d'encodage, et je le formate de façon à n'avoir que deux colonnes : text et label. Label vaudra 1 si c'est un spam, 0 sinon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      0  Go until jurong point, crazy.. Available only ...\n",
      "1      0                      Ok lar... Joking wif u oni...\n",
      "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      0  U dun say so early hor... U c already then say...\n",
      "4      0  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./spam.csv\", encoding=\"latin-1\", usecols=[\"v1\", \"v2\"])\n",
    "df = df.rename(columns={\"v1\": \"label\", \"v2\": \"text\"})\n",
    "df[\"label\"] = df[\"label\"].map({\"spam\": 1, \"ham\": 0})\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je vérifie la distribution des labels. Il y a 13.5% de spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.865937\n",
      "1    0.134063\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "distribution = df['label'].value_counts(normalize = True)\n",
    "print(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On divise le dataset en train (70%), validation (15%) et test (15%). On vérifie la distribution des labels dans les différents datasets pour voir si c'est bien uniforme.\n",
    "\n",
    "Grâce aux seeds, peu importe si on relance le code, les datasets seront identiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution des labels dans le train set : label\n",
      "0    0.865897\n",
      "1    0.134103\n",
      "Name: proportion, dtype: float64\n",
      "Distribution des labels dans le validation set : label\n",
      "0    0.866029\n",
      "1    0.133971\n",
      "Name: proportion, dtype: float64\n",
      "Distribution des labels dans le test set : label\n",
      "0    0.866029\n",
      "1    0.133971\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text, rest_text, train_labels, rest_labels = train_test_split(\n",
    "    df['text'], df['label'], \n",
    "    random_state=1234, # Seed pour la reproductibilité, on évite un biais\n",
    "    test_size=0.3, \n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(\n",
    "    rest_text, rest_labels, \n",
    "    random_state=5678, \n",
    "    test_size=0.5, \n",
    "    stratify=rest_labels\n",
    ")\n",
    "\n",
    "print(\"Distribution des labels dans le train set :\", train_labels.value_counts(normalize=True))\n",
    "print(\"Distribution des labels dans le validation set :\", val_labels.value_counts(normalize=True))\n",
    "print(\"Distribution des labels dans le test set :\", test_labels.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then we gotta do it after that\n"
     ]
    }
   ],
   "source": [
    "# Récupérer le texte de la troisième entrée de train_text\n",
    "print(train_text.iloc[2])  # Affiche le texte à l'index 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On import un modèle BERT pré-entraîné et son tokenizer.\n",
    "bert-base-uncased est un modèle BET de taille standard avec 12 couches, 768 dimensions cachées et 12 têtes d'attention.\n",
    "Toutes les lettres sont en minuscules et les accents sont supprimés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche l'histogramme de la longueur des textes dans le train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 50)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On tokenize le train set, le validation set et le test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lezarman/.virtualenvs/BERT-from-scratch-CDIVNApH/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=25, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "Encoding(num_tokens=25, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "Encoding(num_tokens=25, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length = True,\n",
    "    truncation = True,\n",
    ")\n",
    "\n",
    "print(tokens_train[0])\n",
    "print(tokens_val[0])\n",
    "print(tokens_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée les tenseurs pour le train set, le validation set et le test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'affiche les tenseurs ci-dessous.\n",
    "train_seq, c'est la séquence d'identifiant de tokens pour chaque texte. En gros pour chaque token on cherche dans un index le mot correspondant et on le remplace par son identifiant. \n",
    "\n",
    "Tous les vecteurs ont la même longueur, 25. Cela permet d'éviter la malédiction de la dimensionnalité.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_seq :\n",
      " tensor([[  101,  6694,  1005,  ...,  2611, 11199,   102],\n",
      "        [  101,  2423,  2361,  ...,  1012, 19067,   102],\n",
      "        [  101,  2059,  2057,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  7592, 18237,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2228,  ...,  2412,  2131,   102],\n",
      "        [  101,  7592,  1010,  ...,  2424,  2070,   102]])\n",
      "Premier vecteur train_seq :  tensor([  101,  6694,  1005,  1055, 20661,  2085,   999, 19067,  2102,  2611,\n",
      "         2030,  1038, 29027,  2063,  1004, 24471,  2171,  1004,  2287,  1010,\n",
      "         1041,  2290,  2611, 11199,   102])\n",
      "Troisième vecteur train_seq :  tensor([  101,  2059,  2057, 10657,  2079,  2009,  2044,  2008,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0])\n",
      "Longueur des vecteurs train_seq :  25\n"
     ]
    }
   ],
   "source": [
    "print(\"train_seq :\\n\", train_seq)\n",
    "print(\"Premier vecteur train_seq : \", train_seq[0])\n",
    "print(\"Troisième vecteur train_seq : \", train_seq[2])\n",
    "print(\"Longueur des vecteurs train_seq : \", len(train_seq[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant afficher le mask.\n",
    "train_mask, c'est la séquence d'attention. Si un texte est plus court que les autres, les valeurs non utilisées sont marquées par des 0.\n",
    "Comme vous pouvez le voir juste au dessus, le troisième texte est plus court que le premier, on voit que les 0 du troisième vecteur sont les mêmes que sur son mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mask :\n",
      " tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "Premier vecteur train_mask :  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1])\n",
      "Troisième vecteur train_mask :  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "Longueur des vecteurs train_mask :  25\n"
     ]
    }
   ],
   "source": [
    "print(\"train_mask :\\n\", train_mask)\n",
    "print(\"Premier vecteur train_mask : \", train_mask[0])\n",
    "print(\"Troisième vecteur train_mask : \", train_mask[2])\n",
    "print(\"Longueur des vecteurs train_mask : \", len(train_mask[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tenseur train_y contient le label pour chaque texte.\n",
    "Il est donc de la même longueur que le train set (3900 ici).\n",
    "Toujours, 1 si c'est un spam, 0 sinon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y :\n",
      " tensor([1, 1, 0,  ..., 0, 0, 0])\n",
      "Longueur du vecteur train_y :  3900\n"
     ]
    }
   ],
   "source": [
    "print(\"train_y :\\n\", train_y)\n",
    "print(\"Longueur du vecteur train_y : \", len(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour finir, voici les shapes des tenseurs. Comme on peut le déduire des cellules précédente, ils représentent le nombre des vecteurs et la taille des vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3900, 25])\n",
      "torch.Size([3900, 25])\n",
      "torch.Size([3900])\n"
     ]
    }
   ],
   "source": [
    "print(train_seq.shape)\n",
    "print(train_mask.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant créer un DataLoader pour le train set.\n",
    "\n",
    "Un DataLoader est un itérateur qui permet de parcourir le dataset de manière efficace.\n",
    "\n",
    "Il prend en entrée un dataset et un sampler, et renvoie un batch de données à chaque itération.\n",
    "\n",
    "Le sampler est un objet qui permet de récupérer les indices des données à utiliser pour le batch.\n",
    "\n",
    "RandomSampler est un sampler qui permet de récupérer des indices de manière aléatoire.\n",
    "SequentialSampler est un sampler qui permet de récupérer des indices de manière séquentielle.\n",
    "\n",
    "On définit la taille du batch à 32.\n",
    "\n",
    "D'abord on regroupe les tenseurs dans TensorDataset.\n",
    "\n",
    "Ensuite on crée le RandomSampler pour éviter que l'algorithme apprenne l'ordre des données (ce qui n'a aucun intérêt ici et pourrait mener à une sur-adaptation)\n",
    "\n",
    "Enfin on crée le DataLoader qui va nous permettre de parcourir le dataset.\n",
    "\n",
    "On utilise des batch de 32 données pour optimiser les performances. Si on devait charger les 3900 données en mémoire on serait limité par la mémoire de la machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  101,  6694,  1005,  ...,  2611, 11199,   102],\n",
      "        [  101,  2423,  2361,  ...,  1012, 19067,   102],\n",
      "        [  101,  2059,  2057,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  7592, 18237,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2228,  ...,  2412,  2131,   102],\n",
      "        [  101,  7592,  1010,  ...,  2424,  2070,   102]]), tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), tensor([1, 1, 0,  ..., 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "print(train_data.tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voulons éviter que les paramètres du BERT soient mis à jour dans le processus d'apprentissage.\n",
    "\n",
    "Dans d'autres termes, on veut conserver les apprentissages de BERT durant sa phase de pré-entraînement et les utiliser tel quel dans notre modèle.\n",
    "\n",
    "En effet, avec notre petit dataset, mettre à jour les paramètres de BERT pourrait facilement mener à une sur-adaptation.\n",
    "\n",
    "Pour ce faire, on désactive le calcul des gradients pour les paramètres de BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va utiliser le module nn servant à construire des réseaux de neurones.\n",
    "\n",
    "La classe hérite donc de nn.Module, ce qui est essentiel pour que PyTorch puisse reconnaître notre classe comme un modèle de réseau de neurones.\n",
    "\n",
    "Voici une explication de chaque attribut de la classe :\n",
    "- self.bert : c'est le modèle BERT que nous avons importé et chargé précédemment.\n",
    "- self.dropout : Il s'agit de la couche de Dropout. Elle est appliquée sur les sorties de la couche précédente pour réduire le sur-apprentissage. Dans notre cas, on applique une Dropout de 0.1, ce qui signifie que 10% des neurones sont désactivés (probabilité de 0.1 qu'un neurone donné soit désactivé).\n",
    "- self.relu : Il s'agit de la fonction d'activation ReLU. Elle est appliquée à la sortie de la couche précédente pour introduire une non-linéarité dans le modèle. Sans cela, les réseaux de neurones ne pourraient pas apprendre des fonctions non-linéaires.\n",
    "- self.fc1 : C'est la première couche linéaire (fully connected layer n°1). Elle prend en entrée les sorties de la couche précédente et les transforme en une sortie de taille 512.\n",
    "- self.fc2 : C'est la seconde couche linéaire (fully connected layer n°2). Elle prend en entrée les sorties de la première couche linéaire et les transforme en une sortie de taille 2 (spam ou ham).\n",
    "- self.softmax : Il s'agit de la fonction softmax. Elle est appliquée à la sortie de la seconde couche linéaire pour transformer les valeurs en probabilités. Pour faire simple, elle permet de contenir les valeurs entre 0 et 1 et de s'assurer que la somme des probabilités de spam et ham est égale à 1.\n",
    "\n",
    "Ces attributs sont initialisés dans la méthode __init__.\n",
    "\n",
    "La méthode forward est la méthode qui définit le forward pass du modèle. Elle décrit comment les données sont propagées à travers le modèle. Elle prend en entrée les tenseurs sent_id et mask.\n",
    "- sent_id : sentence id, c'est la séquence d'identifiant de tokens pour chaque texte.\n",
    "- mask : c'est la séquence d'attention. Si un texte est plus court que les autres, les valeurs non utilisées sont marquées par des 0.\n",
    "\n",
    "On applique ensuite la première couche linéaire à la sortie de la couche précédente.\n",
    "\n",
    "Puis on applique la fonction d'activation ReLU.\n",
    "\n",
    "Puis on applique la Dropout.\n",
    "\n",
    "Enfin on applique la seconde couche linéaire.\n",
    "\n",
    "La sortie de cette méthode est la probabilité de spam ou ham pour chaque texte, qu'on rend exploitable grâce à la fonction softmax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "        print(cls_hs.shape)\n",
    "        print(sent_id.shape)\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on instancie notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_Arch(bert)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bon là faut que j'étudie AdamW en vrai, petite pause sur ce notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT-from-scratch-CDIVNApH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
